== The algorithm ==

This algorithm uses the Naive Bayes machine learning algorithm to formulate a set of probabilities to determine the sense of a word. We are trying to find the sense s that maximizes:
P(s | C)
where s is a sense for the word and C is the context of the word. Of course, by Bayes' rule,
P(s | C) = P(C | s) * P(s) / P(C)
but P(C) is constant over all senses, since it does not depend on the sense, so to maximize we can disregard this term; all we care about are the relative amounts. For the same reason, we can take the log of the equation, which gives:
log(P(C | s)) + log(P(s))
Since P(C | s) can be represented by (assuming the naive Bayes assumption of independence):
Π(c in C)P(c | s)
where C, the context, is composed of all the words in the context. So we have:
log(Π(c in C)P(c | s)) + log(P(s)) = ∑(c in C)log(P(c | s)) + log(P(s))
We calculate:
P(c | s) = (1 + (# times c appears in C))/((# of words associated with sense s in training set) + (# of words in the vocabulary for this word))
P(s) = (# times sense s occurs for the word in the training set)/(# times word occurs in the training set as a head word)
The +1 in P(c | s) is to ensure we have no 0 probabilities.
For a simple example of doing Naive Bayes by hand using only co-occurrence, see:
http://www.ims.uni-stuttgart.de/lehre/teaching/2007-SS/stats/wsdex.pdf

The output of learning will be a set of:
P(s) = the "prior" probability of sense s for a given word
P(c | s) = the probability of seeing a particular word in the context given that the sense is s

Train:
	Read in the training data, fetch sense definitions & read in context for each word
	maintain count of:
		number of words c in each context C
		number of different words c that appear in contexts for head word
		number of different words c that appear in contexts for a particular head word sense
		number of times a sense occurs for a head word, number of times a head word occurs in the training set
	register probabilities P(c | s) and P(s) for each sense s and each context word for a given sense s
		use the counts recorded above
Test:
	for each context C and head word w, evaluate the equation:
		s* = argmax(s in set of senses for w) ∑(c in C)log(P(c | s)) + log(P(s))
		output the sense s*

== Why this approach ==

There are several reasons this approach was chosen for our initial algorithm.  Easily implemented, Naive Bayes allows us to get a system up and running quickly, which can then be supplemented with other Machine Learning algorithms if necessary. Additionally, the WSD system that was most accurate in the Senseval-3 English Lexical Sample Task was based on Naive Bayes.

Because Naive Bayes relies exclusively on the probability of features found in the training corpus, Naive Bayes may perform poorly when testing on different corpora.  However, for this project we know both training and testing data will be taken from the same corpus, and so this is less of a consideration.

== Design choices ==

A context C will consist of co-occurrence features of words, but possibly also extra information, such as part-of-speech tags or collocation information. Context consisting of only co-occurrence information is not optimal, but it may be good enough to achieve acceptable disambiguation results.
